{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Todo \n1. Code examples for each type \n2. Pictorial explanation if possible","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T02:21:59.333952Z","iopub.execute_input":"2024-05-10T02:21:59.334536Z","iopub.status.idle":"2024-05-10T02:21:59.341248Z","shell.execute_reply.started":"2024-05-10T02:21:59.334491Z","shell.execute_reply":"2024-05-10T02:21:59.339768Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Cross Validation\nAll different types of Cross validation is written downand possible code will also be taken care of ","metadata":{}},{"cell_type":"markdown","source":"# 1. Leave One (P) Out\nAn exhaustive cross validation technique, 'P' samples are used as the validation set and 'n-p' samples are used as the training set.\n* High computation time\n* Produces good results\n* Not ideal for imbalanced datasets, if all samples eblongs to one class thus it will not generalize well enough","metadata":{}},{"cell_type":"markdown","source":"# 2. K-Fold\nLet k=4, then given dataset will be split into 4 equal parts, where in 4 iterations of validation each part one-at-a-time acts as a test dataset\n* not used for imbalanced dataset\n* Less computation\n* Less time\n* Compartiviely good results","metadata":{}},{"cell_type":"markdown","source":"# 3. Stratified K-fold\nIn case of binary classifcation CAncer/No cancer data will be of SKEWED since less no.of cancer patients, thus the ratio present in the given dataset wrto cancer & no cancer was to be preserved in all the folds.\n* used for Imbalanced dataset","metadata":{}},{"cell_type":"markdown","source":"# 4. Group K-fold\nLet say show wise sales fo all the items has to be predicted, shop will become thr grouping and it cannot be split into train & test has to be in either of the one.\n* A patients photos of canncer X-rays/scans","metadata":{}},{"cell_type":"markdown","source":"# 5. Stratified Group K-fold\nCombination of above two 'Stratified' & 'Group' results in the stratified Group-K-Folds","metadata":{"execution":{"iopub.status.busy":"2024-05-10T02:13:11.481857Z","iopub.execute_input":"2024-05-10T02:13:11.482265Z","iopub.status.idle":"2024-05-10T02:13:11.490990Z","shell.execute_reply.started":"2024-05-10T02:13:11.482233Z","shell.execute_reply":"2024-05-10T02:13:11.489569Z"}}},{"cell_type":"markdown","source":"# 6. Monte Carlo\n* Shuffle Split Cross-validation & repeated random Subsampling cross validation\n* Splitting can be done in percentage of 70-30% or 60-40%\n* Only condition for each iteration is to keep the train-test split percentage different","metadata":{}},{"cell_type":"markdown","source":"# 7. Time Series\nRolling/Forward chain method\n* Since the orde of data is very important\n* The dataset is split into training and validation sets according to time\n* Eg. if you have one year of data then you split can be first 10 montsh are training data and rest two months are your testing data","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}